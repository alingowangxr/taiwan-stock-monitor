# -*- coding: utf-8 -*-
import os, sys, time, random, json, subprocess
import pandas as pd
import yfinance as yf
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# ========== åƒæ•¸èˆ‡è·¯å¾‘è¨­å®š ==========
MARKET_CODE = "cn-share"
DATA_SUBDIR = "dayK"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data", MARKET_CODE, DATA_SUBDIR)
LIST_DIR = os.path.join(BASE_DIR, "data", MARKET_CODE, "lists")
CACHE_LIST_PATH = os.path.join(LIST_DIR, "cn_stock_list_cache.json")

# ğŸ›¡ï¸ ç©©å®šæ€§å„ªå…ˆï¼šä¿æŒ 4 å€‹åŸ·è¡Œç·’ï¼Œé€™æ˜¯å° GitHub Actions æœ€ç©©å®šçš„è¨­å®š
THREADS_CN = 4 
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(LIST_DIR, exist_ok=True)

def log(msg: str):
    print(f"{pd.Timestamp.now():%H:%M:%S}: {msg}")

def ensure_pkg(pkg: str):
    try:
        __import__(pkg)
    except ImportError:
        log(f"ğŸ”§ æ­£åœ¨å®‰è£ {pkg}...")
        subprocess.run([sys.executable, "-m", "pip", "install", "-q", pkg])

def get_cn_list():
    """ç²å– A è‚¡æ¸…å–®ï¼šæ•´åˆ EM æ¥å£èˆ‡å¤šé‡ä¿åº•æ©Ÿåˆ¶"""
    ensure_pkg("akshare")
    import akshare as ak
    threshold = 4500  
    
    # 1. æª¢æŸ¥ä»Šæ—¥å¿«å–
    if os.path.exists(CACHE_LIST_PATH):
        try:
            file_mtime = os.path.getmtime(CACHE_LIST_PATH)
            if datetime.fromtimestamp(file_mtime).date() == datetime.now().date():
                with open(CACHE_LIST_PATH, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    if len(data) >= threshold:
                        log(f"ğŸ“¦ è¼‰å…¥ä»Šæ—¥å¿«å– (å…± {len(data)} æª”)")
                        return data
        except Exception as e:
            log(f"âš ï¸ å¿«å–è®€å–å¤±æ•—: {e}")

    # 2. ç²å–æ¸…å–®
    log("ğŸ“¡ å˜—è©¦å¾ Akshare EM æ¥å£ç²å–æ¸…å–®...")
    try:
        df_sh = ak.stock_sh_a_spot_em()
        df_sz = ak.stock_sz_a_spot_em()
        df = pd.concat([df_sh, df_sz], ignore_index=True)
        
        df['code'] = df['ä»£ç '].astype(str).str.zfill(6)
        valid_prefixes = ('000','001','002','003','300','301','600','601','603','605','688')
        df = df[df['code'].str.startswith(valid_prefixes)]
        
        res = [f"{row['code']}&{row['åç¨±']}" if 'åç¨±' in row else f"{row['code']}&{row['åç§°']}" for _, row in df.iterrows()]
        
        if len(res) >= threshold:
            with open(CACHE_LIST_PATH, "w", encoding="utf-8") as f:
                json.dump(res, f, ensure_ascii=False)
            log(f"âœ… æˆåŠŸç²å– {len(res)} æª”æ¨™çš„")
            return res
    except Exception as e:
        log(f"âš ï¸ EM æ¥å£å¤±æ•—: {e}")

    # 3. æ­·å²å‚™æ´
    if os.path.exists(CACHE_LIST_PATH):
        log("ğŸ”„ æ¥å£å…¨æ•¸å¤±æ•—ï¼Œä½¿ç”¨æ­·å²å¿«å–å‚™æ´...")
        with open(CACHE_LIST_PATH, "r", encoding="utf-8") as f:
            return json.load(f)

    return ["600519&è²´å·èŒ…å°", "000001&å¹³å®‰éŠ€è¡Œ", "300750&å¯§å¾·æ™‚ä»£"]

def download_one(item):
    """å¼·åŒ–ç©©å®šç‰ˆä¸‹è¼‰é‚è¼¯ï¼šé‡å° A è‚¡é¢¨æ§å„ªåŒ–"""
    code, name = item.split('&', 1)
    symbol = f"{code}.SS" if code.startswith('6') else f"{code}.SZ"
    out_path = os.path.join(DATA_DIR, f"{code}_{name}.csv")

    # çºŒè·‘æ©Ÿåˆ¶
    if os.path.exists(out_path) and os.path.getsize(out_path) > 1000:
        return {"status": "exists", "code": code}

    #
